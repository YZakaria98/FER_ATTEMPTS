{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e90e1c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Preprocessing faces...\n",
      "✓ Skipping face detection (dataset already contains faces)\n",
      "✓ This will take 2-3 minutes for the entire dataset\n",
      "\n",
      "Processing train/angry: 3995 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train/angry: 100%|██████████| 3995/3995 [00:02<00:00, 1530.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully processed 3995/3995 images\n",
      "\n",
      "Processing train/disgusted: 436 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train/disgusted: 100%|██████████| 436/436 [00:00<00:00, 1618.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully processed 436/436 images\n",
      "\n",
      "Processing train/fearful: 4097 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train/fearful: 100%|██████████| 4097/4097 [00:02<00:00, 1591.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully processed 4097/4097 images\n",
      "\n",
      "Processing train/happy: 7215 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train/happy: 100%|██████████| 7215/7215 [00:04<00:00, 1605.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully processed 7215/7215 images\n",
      "\n",
      "Processing train/neutral: 4965 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train/neutral: 100%|██████████| 4965/4965 [00:03<00:00, 1605.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully processed 4965/4965 images\n",
      "\n",
      "Processing train/sad: 4830 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train/sad: 100%|██████████| 4830/4830 [00:02<00:00, 1634.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully processed 4830/4830 images\n",
      "\n",
      "Processing train/surprised: 3171 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train/surprised: 100%|██████████| 3171/3171 [00:02<00:00, 1506.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully processed 3171/3171 images\n",
      "\n",
      "Processing test/angry: 958 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test/angry: 100%|██████████| 958/958 [00:00<00:00, 1608.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully processed 958/958 images\n",
      "\n",
      "Processing test/disgusted: 111 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test/disgusted: 100%|██████████| 111/111 [00:00<00:00, 1662.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully processed 111/111 images\n",
      "\n",
      "Processing test/fearful: 1024 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test/fearful: 100%|██████████| 1024/1024 [00:00<00:00, 1540.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully processed 1024/1024 images\n",
      "\n",
      "Processing test/happy: 1774 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test/happy: 100%|██████████| 1774/1774 [00:01<00:00, 1611.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully processed 1774/1774 images\n",
      "\n",
      "Processing test/neutral: 1233 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test/neutral: 100%|██████████| 1233/1233 [00:00<00:00, 1629.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully processed 1233/1233 images\n",
      "\n",
      "Processing test/sad: 1247 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test/sad: 100%|██████████| 1247/1247 [00:00<00:00, 1606.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully processed 1247/1247 images\n",
      "\n",
      "Processing test/surprised: 831 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test/surprised: 100%|██████████| 831/831 [00:00<00:00, 1571.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully processed 831/831 images\n",
      "\n",
      "✓ Preprocessing complete! Cropped faces saved to: C:/adam/AMIT_Diploma/grad_project/preprocessed_faces\n",
      "\n",
      "Step 2: Extracting features...\n",
      "\n",
      "Extracting features for train set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 898/898 [06:31<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting features for test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:43<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features saved to: C:/adam/AMIT_Diploma/grad_project/extracted_features.pt\n",
      "Feature dimension: 1280\n",
      "Loading pre-extracted features...\n",
      "Feature dimension: 1280\n",
      "Train samples: 25838\n",
      "Val samples: 2871\n",
      "Test samples: 7178\n",
      "\n",
      "Starting training...\n",
      "\n",
      "--- Epoch 1/50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 404/404 [00:02<00:00, 169.43it/s, Loss=1.4350, Acc=0.4709]\n",
      "Validation: 100%|██████████| 45/45 [00:00<00:00, 868.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4026, Train Acc: 47.09%\n",
      "Val Loss: 1.2686, Val Acc: 51.27%\n",
      "✓ New best model saved! (Val Acc: 51.27%)\n",
      "\n",
      "--- Epoch 2/50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 404/404 [00:02<00:00, 177.68it/s, Loss=1.1193, Acc=0.5351]\n",
      "Validation: 100%|██████████| 45/45 [00:00<00:00, 1188.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2340, Train Acc: 53.51%\n",
      "Val Loss: 1.2233, Val Acc: 54.75%\n",
      "✓ New best model saved! (Val Acc: 54.75%)\n",
      "\n",
      "--- Epoch 3/50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 404/404 [00:02<00:00, 169.02it/s, Loss=1.2269, Acc=0.5679]\n",
      "Validation: 100%|██████████| 45/45 [00:00<00:00, 1006.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1505, Train Acc: 56.79%\n",
      "Val Loss: 1.1925, Val Acc: 56.36%\n",
      "✓ New best model saved! (Val Acc: 56.36%)\n",
      "\n",
      "--- Epoch 4/50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 404/404 [00:02<00:00, 171.90it/s, Loss=1.2097, Acc=0.5974]\n",
      "Validation: 100%|██████████| 45/45 [00:00<00:00, 900.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0805, Train Acc: 59.74%\n",
      "Val Loss: 1.1746, Val Acc: 56.67%\n",
      "✓ New best model saved! (Val Acc: 56.67%)\n",
      "\n",
      "--- Epoch 5/50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 404/404 [00:02<00:00, 167.14it/s, Loss=1.0714, Acc=0.6215]\n",
      "Validation: 100%|██████████| 45/45 [00:00<00:00, 894.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0183, Train Acc: 62.15%\n",
      "Val Loss: 1.1720, Val Acc: 57.26%\n",
      "✓ New best model saved! (Val Acc: 57.26%)\n",
      "\n",
      "--- Epoch 6/50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 404/404 [00:02<00:00, 149.95it/s, Loss=0.8093, Acc=0.6435]\n",
      "Validation: 100%|██████████| 45/45 [00:00<00:00, 861.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9570, Train Acc: 64.35%\n",
      "Val Loss: 1.1687, Val Acc: 57.30%\n",
      "✓ New best model saved! (Val Acc: 57.30%)\n",
      "\n",
      "--- Epoch 7/50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 404/404 [00:02<00:00, 166.46it/s, Loss=0.6403, Acc=0.6658]\n",
      "Validation: 100%|██████████| 45/45 [00:00<00:00, 795.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8948, Train Acc: 66.58%\n",
      "Val Loss: 1.1896, Val Acc: 57.09%\n",
      "\n",
      "--- Epoch 8/50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 404/404 [00:02<00:00, 163.66it/s, Loss=1.1001, Acc=0.6862]\n",
      "Validation: 100%|██████████| 45/45 [00:00<00:00, 1025.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8401, Train Acc: 68.62%\n",
      "Val Loss: 1.2040, Val Acc: 58.03%\n",
      "✓ New best model saved! (Val Acc: 58.03%)\n",
      "\n",
      "--- Epoch 9/50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 404/404 [00:02<00:00, 159.51it/s, Loss=0.7680, Acc=0.7085]\n",
      "Validation: 100%|██████████| 45/45 [00:00<00:00, 918.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7850, Train Acc: 70.85%\n",
      "Val Loss: 1.2352, Val Acc: 57.96%\n",
      "\n",
      "--- Epoch 10/50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 404/404 [00:02<00:00, 162.17it/s, Loss=0.9557, Acc=0.7347]\n",
      "Validation: 100%|██████████| 45/45 [00:00<00:00, 1114.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7243, Train Acc: 73.47%\n",
      "Val Loss: 1.2654, Val Acc: 58.13%\n",
      "✓ New best model saved! (Val Acc: 58.13%)\n",
      "\n",
      "--- Epoch 11/50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 404/404 [00:02<00:00, 164.88it/s, Loss=0.9868, Acc=0.7491]\n",
      "Validation: 100%|██████████| 45/45 [00:00<00:00, 1001.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6829, Train Acc: 74.91%\n",
      "Val Loss: 1.3018, Val Acc: 57.61%\n",
      "\n",
      "--- Epoch 12/50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 404/404 [00:02<00:00, 173.72it/s, Loss=0.6481, Acc=0.7668]\n",
      "Validation: 100%|██████████| 45/45 [00:00<00:00, 894.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6319, Train Acc: 76.68%\n",
      "Val Loss: 1.3171, Val Acc: 59.04%\n",
      "✓ New best model saved! (Val Acc: 59.04%)\n",
      "\n",
      "--- Epoch 13/50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 404/404 [00:02<00:00, 173.65it/s, Loss=1.0868, Acc=0.7833]\n",
      "Validation: 100%|██████████| 45/45 [00:00<00:00, 964.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5839, Train Acc: 78.33%\n",
      "Val Loss: 1.3885, Val Acc: 59.00%\n",
      "\n",
      "--- Epoch 14/50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 404/404 [00:02<00:00, 159.67it/s, Loss=0.5608, Acc=0.7979]\n",
      "Validation: 100%|██████████| 45/45 [00:00<00:00, 963.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5470, Train Acc: 79.79%\n",
      "Val Loss: 1.4616, Val Acc: 58.38%\n",
      "\n",
      "--- Epoch 15/50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 404/404 [00:02<00:00, 159.01it/s, Loss=0.4056, Acc=0.8117]\n",
      "Validation: 100%|██████████| 45/45 [00:00<00:00, 789.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5108, Train Acc: 81.17%\n",
      "Val Loss: 1.4421, Val Acc: 58.86%\n",
      "\n",
      "--- Epoch 16/50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 404/404 [00:02<00:00, 147.11it/s, Loss=0.7739, Acc=0.8248]\n",
      "Validation: 100%|██████████| 45/45 [00:00<00:00, 725.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4744, Train Acc: 82.48%\n",
      "Val Loss: 1.4996, Val Acc: 58.73%\n",
      "\n",
      "--- Epoch 17/50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 404/404 [00:02<00:00, 169.75it/s, Loss=0.6071, Acc=0.8357]\n",
      "Validation: 100%|██████████| 45/45 [00:00<00:00, 1212.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4530, Train Acc: 83.57%\n",
      "Val Loss: 1.5198, Val Acc: 59.60%\n",
      "✓ New best model saved! (Val Acc: 59.60%)\n",
      "\n",
      "--- Epoch 18/50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 404/404 [00:02<00:00, 168.83it/s, Loss=0.5182, Acc=0.8442]\n",
      "Validation: 100%|██████████| 45/45 [00:00<00:00, 995.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4240, Train Acc: 84.42%\n",
      "Val Loss: 1.5653, Val Acc: 59.63%\n",
      "✓ New best model saved! (Val Acc: 59.63%)\n",
      "\n",
      "--- Epoch 19/50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 404/404 [00:02<00:00, 170.40it/s, Loss=0.7136, Acc=0.8583]\n",
      "Validation: 100%|██████████| 45/45 [00:00<00:00, 913.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3939, Train Acc: 85.83%\n",
      "Val Loss: 1.5887, Val Acc: 59.84%\n",
      "✓ New best model saved! (Val Acc: 59.84%)\n",
      "\n",
      "--- Epoch 20/50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 404/404 [00:02<00:00, 161.14it/s, Loss=0.3815, Acc=0.8618]\n",
      "Validation: 100%|██████████| 45/45 [00:00<00:00, 966.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3829, Train Acc: 86.18%\n",
      "Val Loss: 1.5961, Val Acc: 58.38%\n",
      "\n",
      "--- Epoch 21/50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 404/404 [00:02<00:00, 157.49it/s, Loss=0.1870, Acc=0.8728]\n",
      "Validation: 100%|██████████| 45/45 [00:00<00:00, 925.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3516, Train Acc: 87.28%\n",
      "Val Loss: 1.6710, Val Acc: 58.86%\n",
      "\n",
      "--- Epoch 22/50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 404/404 [00:02<00:00, 165.49it/s, Loss=0.2505, Acc=0.8736]\n",
      "Validation: 100%|██████████| 45/45 [00:00<00:00, 786.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3455, Train Acc: 87.36%\n",
      "Val Loss: 1.7234, Val Acc: 59.74%\n",
      "\n",
      "--- Epoch 23/50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 404/404 [00:02<00:00, 165.15it/s, Loss=0.4206, Acc=0.8826]\n",
      "Validation: 100%|██████████| 45/45 [00:00<00:00, 1026.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3218, Train Acc: 88.26%\n",
      "Val Loss: 1.7652, Val Acc: 57.65%\n",
      "\n",
      "--- Epoch 24/50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 404/404 [00:02<00:00, 154.47it/s, Loss=0.4428, Acc=0.8879]\n",
      "Validation: 100%|██████████| 45/45 [00:00<00:00, 817.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3121, Train Acc: 88.79%\n",
      "Val Loss: 1.7921, Val Acc: 58.13%\n",
      "\n",
      "Early stopping triggered after 24 epochs\n",
      "\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms, datasets, models\n",
    "from retinaface import RetinaFace\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from glob import glob\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: ONE-TIME PREPROCESSING - Run this ONCE and save results\n",
    "# ============================================================================\n",
    "\n",
    "def preprocess_and_save_faces(data_path, output_path, target_size=(48, 48), skip_detection=True):\n",
    "    \"\"\"\n",
    "    Detect faces once and save cropped faces to disk.\n",
    "    This should be run ONCE before training.\n",
    "    \n",
    "    Args:\n",
    "        skip_detection: If True, skips face detection entirely (fastest, for pre-cropped datasets like FER2013).\n",
    "    \"\"\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    if skip_detection:\n",
    "        print(\"✓ Skipping face detection (dataset already contains faces)\")\n",
    "        print(\"✓ This will take 2-3 minutes for the entire dataset\\n\")\n",
    "    \n",
    "    # Initialize face detector if needed\n",
    "    face_cascade = None\n",
    "    if not skip_detection:\n",
    "        print(\"✓ Using OpenCV Haar Cascade for face detection\")\n",
    "        face_cascade = cv2.CascadeClassifier(\n",
    "            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "        )\n",
    "    \n",
    "    for split in ['train', 'test']:\n",
    "        split_path = os.path.join(data_path, split)\n",
    "        output_split_path = os.path.join(output_path, split)\n",
    "        \n",
    "        classes = os.listdir(split_path)\n",
    "        \n",
    "        for emotion_class in classes:\n",
    "            class_path = os.path.join(split_path, emotion_class)\n",
    "            output_class_path = os.path.join(output_split_path, emotion_class)\n",
    "            os.makedirs(output_class_path, exist_ok=True)\n",
    "            \n",
    "            image_files = glob(os.path.join(class_path, \"*.png\"))\n",
    "            \n",
    "            print(f\"Processing {split}/{emotion_class}: {len(image_files)} images\")\n",
    "            \n",
    "            successful = 0\n",
    "            for img_path in tqdm(image_files, desc=f\"{split}/{emotion_class}\"):\n",
    "                try:\n",
    "                    img = Image.open(img_path).convert(\"RGB\")\n",
    "                    img_np = np.array(img)\n",
    "                    \n",
    "                    if skip_detection:\n",
    "                        # Just resize - no detection needed\n",
    "                        crop = cv2.resize(img_np, target_size)\n",
    "                    else:\n",
    "                        # OpenCV Haar Cascade detection\n",
    "                        gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n",
    "                        faces = face_cascade.detectMultiScale(\n",
    "                            gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30)\n",
    "                        )\n",
    "                        \n",
    "                        if len(faces) > 0:\n",
    "                            # Use the largest face\n",
    "                            x, y, w, h = max(faces, key=lambda f: f[2] * f[3])\n",
    "                            crop = img_np[y:y+h, x:x+w]\n",
    "                            crop = cv2.resize(crop, target_size)\n",
    "                        else:\n",
    "                            crop = cv2.resize(img_np, target_size)  # Fallback\n",
    "                    \n",
    "                    # Save cropped face\n",
    "                    crop_pil = Image.fromarray(crop)\n",
    "                    output_file = os.path.join(output_class_path, os.path.basename(img_path))\n",
    "                    crop_pil.save(output_file)\n",
    "                    successful += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    pass  # Skip problematic images\n",
    "            \n",
    "            print(f\"✓ Successfully processed {successful}/{len(image_files)} images\\n\")\n",
    "    \n",
    "    print(\"✓ Preprocessing complete! Cropped faces saved to:\", output_path)\n",
    "\n",
    "\n",
    "def extract_and_save_features(preprocessed_path, output_file, device='cpu'):\n",
    "    \"\"\"\n",
    "    Extract features from all preprocessed faces ONCE and save to disk.\n",
    "    This should be run ONCE after preprocessing faces.\n",
    "    \"\"\"\n",
    "    # Load pre-trained feature extractor\n",
    "    feature_extractor = models.efficientnet_b0(\n",
    "        weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1\n",
    "    )\n",
    "    feature_extractor = nn.Sequential(*list(feature_extractor.children())[:-1])  # Remove classifier\n",
    "    feature_extractor.eval()\n",
    "    feature_extractor.to(device)\n",
    "    \n",
    "    # Transformation for feature extraction\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # EfficientNet input size\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    features_dict = {'train': [], 'test': []}\n",
    "    \n",
    "    for split in ['train', 'test']:\n",
    "        dataset = datasets.ImageFolder(\n",
    "            os.path.join(preprocessed_path, split),\n",
    "            transform=transform\n",
    "        )\n",
    "        dataloader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "        \n",
    "        split_features = []\n",
    "        split_labels = []\n",
    "        \n",
    "        print(f\"\\nExtracting features for {split} set...\")\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(dataloader):\n",
    "                images = images.to(device)\n",
    "                features = feature_extractor(images)\n",
    "                features = features.squeeze(-1).squeeze(-1)  # Remove spatial dimensions\n",
    "                \n",
    "                split_features.append(features.cpu())\n",
    "                split_labels.append(labels)\n",
    "        \n",
    "        features_dict[split] = {\n",
    "            'features': torch.cat(split_features),\n",
    "            'labels': torch.cat(split_labels),\n",
    "            'classes': dataset.classes\n",
    "        }\n",
    "    \n",
    "    # Save features\n",
    "    torch.save(features_dict, output_file)\n",
    "    print(f\"\\nFeatures saved to: {output_file}\")\n",
    "    print(f\"Feature dimension: {features_dict['train']['features'].shape[1]}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: FAST TRAINING DATASET - Uses pre-extracted features\n",
    "# ============================================================================\n",
    "\n",
    "class PreExtractedFeatureDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Fast dataset that loads pre-extracted features from memory/disk.\n",
    "    \"\"\"\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: TRAINING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "class FER_EfficientNetClassifier(nn.Module):\n",
    "    def __init__(self, feature_dim, num_classes=7):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, \n",
    "                num_epochs, device='cpu', patience=5):\n",
    "    \"\"\"\n",
    "    Fast training function with early stopping.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{num_epochs} ---\")\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        train_bar = tqdm(train_loader, desc='Training')\n",
    "        for features, labels in train_bar:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * features.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "            \n",
    "            train_bar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Acc': f'{train_correct/train_total:.4f}'\n",
    "            })\n",
    "        \n",
    "        train_loss /= train_total\n",
    "        train_acc = train_correct / train_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for features, labels in tqdm(val_loader, desc='Validation'):\n",
    "                features, labels = features.to(device), labels.to(device)\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item() * features.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "        \n",
    "        val_loss /= val_total\n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc*100:.2f}%\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"✓ New best model saved! (Val Acc: {val_acc*100:.2f}%)\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: MAIN TRAINING SCRIPT\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    # Paths\n",
    "    DATA_PATH = \"C:/adam/AMIT_Diploma/grad_project/archive (1)\"\n",
    "    PREPROCESSED_PATH = \"C:/adam/AMIT_Diploma/grad_project/preprocessed_faces\"\n",
    "    FEATURES_FILE = \"C:/adam/AMIT_Diploma/grad_project/extracted_features.pt\"\n",
    "    \n",
    "    # ========================================================================\n",
    "    # OPTION A: First time setup (run once)\n",
    "    # ========================================================================\n",
    "    # FASTEST (5-8 minutes) - Skip face detection for FER2013\n",
    "    print(\"Step 1: Preprocessing faces...\")\n",
    "    preprocess_and_save_faces(DATA_PATH, PREPROCESSED_PATH, skip_detection=True)\n",
    "    # \n",
    "    print(\"\\nStep 2: Extracting features...\")\n",
    "    extract_and_save_features(PREPROCESSED_PATH, FEATURES_FILE)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # OPTION B: Fast training (run every time after preprocessing)\n",
    "    # ========================================================================\n",
    "    print(\"Loading pre-extracted features...\")\n",
    "    features_dict = torch.load(FEATURES_FILE)\n",
    "    \n",
    "    # Get feature dimension\n",
    "    feature_dim = features_dict['train']['features'].shape[1]\n",
    "    print(f\"Feature dimension: {feature_dim}\")\n",
    "    \n",
    "    # Split train into train/val\n",
    "    train_features = features_dict['train']['features']\n",
    "    train_labels = features_dict['train']['labels']\n",
    "    \n",
    "    # Stratified split\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
    "    train_idx, val_idx = next(splitter.split(\n",
    "        np.arange(len(train_labels)),\n",
    "        train_labels.numpy()\n",
    "    ))\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = PreExtractedFeatureDataset(\n",
    "        train_features[train_idx],\n",
    "        train_labels[train_idx]\n",
    "    )\n",
    "    val_dataset = PreExtractedFeatureDataset(\n",
    "        train_features[val_idx],\n",
    "        train_labels[val_idx]\n",
    "    )\n",
    "    test_dataset = PreExtractedFeatureDataset(\n",
    "        features_dict['test']['features'],\n",
    "        features_dict['test']['labels']\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    print(f\"Train samples: {len(train_dataset)}\")\n",
    "    print(f\"Val samples: {len(val_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = FER_EfficientNetClassifier(feature_dim=feature_dim, num_classes=7)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\nStarting training...\")\n",
    "    trained_model = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer,\n",
    "        num_epochs=50, device=device, patience=5\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining complete!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d779cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_grad (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
