{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab2516fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "WARNING: CPU training - will take 6-10 hours\n",
      "\n",
      "============================================================\n",
      "Loading dataset...\n",
      "============================================================\n",
      "\n",
      "Dataset Statistics:\n",
      "  Total images: 35887\n",
      "  Classes (7): ['angry', 'disgusted', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n",
      "\n",
      "  Class distribution:\n",
      "    angry: 4953\n",
      "    disgusted: 547\n",
      "    fearful: 5121\n",
      "    happy: 8989\n",
      "    neutral: 6198\n",
      "    sad: 6077\n",
      "    surprised: 4002\n",
      "\n",
      "Creating stratified split (val_size=0.15)...\n",
      "  Train samples: 30503\n",
      "  Val samples: 5384\n",
      "Pre-loading 30503 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching: 100%|██████████| 30503/30503 [03:44<00:00, 136.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loading 5384 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching: 100%|██████████| 5384/5384 [00:51<00:00, 105.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Building enhanced model...\n",
      "============================================================\n",
      "  Model: resnet34\n",
      "  Architecture: RESNET34 + 3-layer classifier\n",
      "  Trainable params: 22,342,471\n",
      "\n",
      "  Class weights: [ 2114.7783 19146.703   2045.306   1165.3425  1690.0564  1723.7594\n",
      "  2617.0537]\n",
      "  Loss: Label Smoothing CE (eps=0.1)\n",
      "\n",
      "============================================================\n",
      "Starting high-accuracy training...\n",
      "Mixed Precision: DISABLED (CPU mode)\n",
      "Target: 75-80% validation accuracy\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 1/60]\n",
      "  Train -> Loss: 4214.5147  Acc: 25.67%\n",
      "  Val   -> Loss: 3533.6334  Acc: 49.18%\n",
      "  Gap: -23.51% | LR: 1.20e-04\n",
      "  ✓ New best! Val Acc: 49.18% (Gap: -23.51%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 2/60]\n",
      "  Train -> Loss: 3818.4594  Acc: 41.24%\n",
      "  Val   -> Loss: 3408.6219  Acc: 54.42%\n",
      "  Gap: -13.18% | LR: 1.20e-04\n",
      "  ✓ New best! Val Acc: 54.42% (Gap: -13.18%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 701\u001b[39m\n\u001b[32m    697\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Model saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG[\u001b[33m'\u001b[39m\u001b[33msave_dir\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/best_model.pth\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    700\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 683\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    680\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Loss: Standard Cross Entropy\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    682\u001b[39m \u001b[38;5;66;03m# ============= Training =============\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m683\u001b[39m best_val_acc, best_epoch = \u001b[43mtrain_advanced\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_names\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[38;5;66;03m# ============= Final Results =============\u001b[39;00m\n\u001b[32m    688\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 473\u001b[39m, in \u001b[36mtrain_advanced\u001b[39m\u001b[34m(model, train_loader, val_loader, criterion, device, CONFIG, class_names)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, CONFIG[\u001b[33m'\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m'\u001b[39m] + \u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     train_loss, train_acc = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_mixup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muse_mixup\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmixup_alpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmixup_alpha\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_amp\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_amp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cutmix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muse_cutmix\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m     \u001b[38;5;66;03m# Use TTA after epoch 20 for better validation accuracy\u001b[39;00m\n\u001b[32m    481\u001b[39m     use_tta = CONFIG[\u001b[33m'\u001b[39m\u001b[33muse_tta\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m epoch > \u001b[32m20\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 360\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, criterion, optimizer, device, epoch, scaler, use_mixup, mixup_alpha, use_amp, use_cutmix)\u001b[39m\n\u001b[32m    357\u001b[39m     correct += (preds == labels).sum().item()\n\u001b[32m    359\u001b[39m optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m360\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    361\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m    362\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\adam\\AMIT_Diploma\\grad_project\\.venv_grad\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\adam\\AMIT_Diploma\\grad_project\\.venv_grad\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\adam\\AMIT_Diploma\\grad_project\\.venv_grad\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "High-Accuracy FER Training Pipeline targeting 75-80% Val Accuracy:\n",
    " - ResNet34 with better architecture\n",
    " - Label smoothing for better generalization\n",
    " - Advanced augmentations (CutOut, stronger transforms)\n",
    " - Knowledge distillation-ready features\n",
    " - Better regularization to prevent overfitting\n",
    " - Extended training with curriculum learning\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  1. Label Smoothing Loss\n",
    "# ============================================================\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"\"\"\n",
    "    Label smoothing prevents overconfident predictions and improves generalization.\n",
    "    This is crucial for reaching 75-80% accuracy.\n",
    "    \"\"\"\n",
    "    def __init__(self, epsilon: float = 0.1, weight=None):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.weight = weight\n",
    "        \n",
    "    def forward(self, outputs, targets):\n",
    "        n_classes = outputs.size(-1)\n",
    "        log_preds = F.log_softmax(outputs, dim=-1)\n",
    "        \n",
    "        # Smooth labels\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(log_preds)\n",
    "            true_dist.fill_(self.epsilon / (n_classes - 1))\n",
    "            true_dist.scatter_(1, targets.unsqueeze(1), 1.0 - self.epsilon)\n",
    "            \n",
    "            # Apply class weights if provided\n",
    "            if self.weight is not None:\n",
    "                true_dist = true_dist * self.weight.unsqueeze(0)\n",
    "        \n",
    "        loss = torch.sum(-true_dist * log_preds, dim=-1)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  2. Advanced Mixup with CutMix\n",
    "# ============================================================\n",
    "\n",
    "def mixup_data(x, y, alpha=0.4, device='cuda'):\n",
    "    \"\"\"Enhanced mixup for better generalization\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "        lam = max(lam, 1 - lam)  # Ensure lam >= 0.5 for stability\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def cutmix_data(x, y, alpha=1.0, device='cuda'):\n",
    "    \"\"\"\n",
    "    CutMix augmentation: randomly cuts and pastes patches between images.\n",
    "    Better for facial features than standard mixup.\n",
    "    \"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "\n",
    "    # Get random box\n",
    "    W, H = x.size(2), x.size(3)\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "\n",
    "    # Uniform sampling\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    # Apply cutmix\n",
    "    x_cutmix = x.clone()\n",
    "    x_cutmix[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]\n",
    "    \n",
    "    # Adjust lambda to exactly match pixel ratio\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (W * H))\n",
    "    \n",
    "    y_a, y_b = y, y[index]\n",
    "    return x_cutmix, y_a, y_b, lam\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  3. Data Collection\n",
    "# ============================================================\n",
    "\n",
    "def gather_image_paths_and_labels(root_dir: str) -> Tuple[List[str], List[int], List[str]]:\n",
    "    \"\"\"Collect all images from emotion class subdirectories.\"\"\"\n",
    "    root = Path(root_dir)\n",
    "    if not root.exists():\n",
    "        raise FileNotFoundError(f\"Dataset root not found: {root_dir}\")\n",
    "\n",
    "    image_data = []\n",
    "    exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\"}\n",
    "    \n",
    "    for img_path in root.rglob(\"*\"):\n",
    "        if img_path.suffix.lower() in exts:\n",
    "            emotion_class = img_path.parent.name\n",
    "            if emotion_class in ['train', 'test']:\n",
    "                emotion_class = img_path.parent.parent.name\n",
    "            \n",
    "            image_data.append((str(img_path), emotion_class))\n",
    "    \n",
    "    if not image_data:\n",
    "        raise ValueError(f\"No images found under {root_dir}\")\n",
    "    \n",
    "    unique_classes = sorted(set(emotion for _, emotion in image_data))\n",
    "    class_to_idx = {name: idx for idx, name in enumerate(unique_classes)}\n",
    "    \n",
    "    image_paths = [path for path, _ in image_data]\n",
    "    labels = [class_to_idx[emotion] for _, emotion in image_data]\n",
    "    \n",
    "    return image_paths, labels, unique_classes\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  4. Enhanced Pre-cached Dataset\n",
    "# ============================================================\n",
    "\n",
    "class PreCachedImageDataset(Dataset):\n",
    "    \"\"\"Ultra-fast dataset with pre-cached tensors.\"\"\"\n",
    "    def __init__(self, image_paths: List[str], labels: List[int], \n",
    "                 transform=None, cache_images: bool = True, img_size: int = 224,\n",
    "                 is_train: bool = True):\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "        self.cached_tensors = []\n",
    "        \n",
    "        if cache_images:\n",
    "            print(f\"Pre-loading {len(image_paths)} images...\")\n",
    "            \n",
    "            imagenet_mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "            imagenet_std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "            \n",
    "            resize_transform = transforms.Compose([\n",
    "                transforms.Resize(img_size),\n",
    "                transforms.ToTensor(),\n",
    "            ])\n",
    "            \n",
    "            for path in tqdm(image_paths, desc=\"Caching\"):\n",
    "                try:\n",
    "                    img = Image.open(path).convert(\"RGB\")\n",
    "                    img_tensor = resize_transform(img)\n",
    "                    \n",
    "                    if not is_train:\n",
    "                        img_tensor = (img_tensor - imagenet_mean) / imagenet_std\n",
    "                    \n",
    "                    self.cached_tensors.append(img_tensor)\n",
    "                except Exception as e:\n",
    "                    blank = torch.zeros(3, img_size, img_size)\n",
    "                    if not is_train:\n",
    "                        blank = (blank - imagenet_mean) / imagenet_std\n",
    "                    self.cached_tensors.append(blank)\n",
    "        else:\n",
    "            self.cached_tensors = None\n",
    "            self.image_paths = image_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.cached_tensors is not None:\n",
    "            img = self.cached_tensors[idx]\n",
    "            if self.is_train and self.transform:\n",
    "                img = self.transform(img)\n",
    "        else:\n",
    "            img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "        \n",
    "        return img, self.labels[idx]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  5. Advanced Transforms\n",
    "# ============================================================\n",
    "\n",
    "def get_augmentation_transforms() -> Tuple[transforms.Compose, transforms.Compose]:\n",
    "    \"\"\"\n",
    "    Strong augmentation strategy for better generalization.\n",
    "    \"\"\"\n",
    "    imagenet_mean = [0.485, 0.456, 0.406]\n",
    "    imagenet_std = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    # Training: Strong augmentation\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(20),  # Increased rotation\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.1),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.15, 0.15), scale=(0.85, 1.15)),\n",
    "        transforms.RandomPerspective(distortion_scale=0.2, p=0.3),  # Add perspective\n",
    "        transforms.Normalize(imagenet_mean, imagenet_std),\n",
    "        transforms.RandomErasing(p=0.3, scale=(0.02, 0.2)),  # Stronger erasing\n",
    "    ])\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Normalize(imagenet_mean, imagenet_std),\n",
    "    ])\n",
    "    \n",
    "    return train_transform, val_transform\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  6. Enhanced Model with Attention\n",
    "# ============================================================\n",
    "\n",
    "def build_model(model_name: str = 'resnet34', num_classes: int = 7, \n",
    "                dropout_rate: float = 0.5) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Build enhanced ResNet with better classifier.\n",
    "    Note: SE attention is added to layer4 of the backbone, not the fc layer.\n",
    "    \"\"\"\n",
    "    if model_name == 'resnet18':\n",
    "        weights = models.ResNet18_Weights.IMAGENET1K_V1\n",
    "        model = models.resnet18(weights=weights)\n",
    "    elif model_name == 'resnet34':\n",
    "        weights = models.ResNet34_Weights.IMAGENET1K_V1\n",
    "        model = models.resnet34(weights=weights)\n",
    "    elif model_name == 'resnet50':\n",
    "        weights = models.ResNet50_Weights.IMAGENET1K_V1\n",
    "        model = models.resnet50(weights=weights)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "\n",
    "    in_features = model.fc.in_features\n",
    "    \n",
    "    # Enhanced 3-layer classifier for better capacity\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.BatchNorm1d(in_features),\n",
    "        nn.Dropout(dropout_rate * 0.5),\n",
    "        nn.Linear(in_features, 1024),\n",
    "        nn.BatchNorm1d(1024),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(dropout_rate),\n",
    "        nn.Linear(1024, 512),\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(dropout_rate),\n",
    "        nn.Linear(512, num_classes)\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  7. Training Functions with Advanced Augmentation\n",
    "# ============================================================\n",
    "\n",
    "def train_one_epoch(model: nn.Module, loader: DataLoader, criterion: nn.Module,\n",
    "                   optimizer: optim.Optimizer, device: torch.device, epoch: int,\n",
    "                   scaler: GradScaler = None, use_mixup: bool = True, \n",
    "                   mixup_alpha: float = 0.4, use_amp: bool = False,\n",
    "                   use_cutmix: bool = True) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Training with mixup/cutmix alternation for better generalization.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch} [Train]\", leave=False)\n",
    "\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        # Randomly choose between mixup and cutmix\n",
    "        use_aug = random.random() < 0.5\n",
    "        use_cutmix_now = use_cutmix and random.random() < 0.5\n",
    "        \n",
    "        if use_amp and scaler is not None:\n",
    "            with autocast(device_type='cuda'):\n",
    "                if use_aug and use_mixup:\n",
    "                    if use_cutmix_now:\n",
    "                        mixed_images, labels_a, labels_b, lam = cutmix_data(images, labels, 1.0, device)\n",
    "                    else:\n",
    "                        mixed_images, labels_a, labels_b, lam = mixup_data(images, labels, mixup_alpha, device)\n",
    "                    \n",
    "                    outputs = model(mixed_images)\n",
    "                    loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
    "                    \n",
    "                    preds = outputs.argmax(dim=1)\n",
    "                    correct += (lam * (preds == labels_a).sum().item() + (1 - lam) * (preds == labels_b).sum().item())\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    preds = outputs.argmax(dim=1)\n",
    "                    correct += (preds == labels).sum().item()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            if use_aug and use_mixup:\n",
    "                if use_cutmix_now:\n",
    "                    mixed_images, labels_a, labels_b, lam = cutmix_data(images, labels, 1.0, device)\n",
    "                else:\n",
    "                    mixed_images, labels_a, labels_b, lam = mixup_data(images, labels, mixup_alpha, device)\n",
    "                \n",
    "                outputs = model(mixed_images)\n",
    "                loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
    "                \n",
    "                preds = outputs.argmax(dim=1)\n",
    "                correct += (lam * (preds == labels_a).sum().item() + (1 - lam) * (preds == labels_b).sum().item())\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                preds = outputs.argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        total += images.size(0)\n",
    "\n",
    "        if pbar.n % max(1, len(loader) // 20) == 0:\n",
    "            avg_loss = running_loss / (pbar.n + 1)\n",
    "            avg_acc = 100.0 * correct / total\n",
    "            pbar.set_postfix({\"loss\": f\"{avg_loss:.4f}\", \"acc\": f\"{avg_acc:.2f}%\"})\n",
    "\n",
    "    avg_loss = running_loss / len(loader)\n",
    "    avg_acc = 100.0 * correct / total\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader, criterion: nn.Module,\n",
    "            device: torch.device, epoch: int, use_amp: bool = False,\n",
    "            use_tta: bool = False) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Evaluation with optional Test-Time Augmentation.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch} [Val]\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in pbar:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            if use_amp:\n",
    "                with autocast(device_type='cuda'):\n",
    "                    if use_tta:\n",
    "                        # Test-Time Augmentation: horizontal flip\n",
    "                        outputs1 = model(images)\n",
    "                        outputs2 = model(torch.flip(images, dims=[3]))\n",
    "                        outputs = (outputs1 + outputs2) / 2\n",
    "                    else:\n",
    "                        outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "            else:\n",
    "                if use_tta:\n",
    "                    outputs1 = model(images)\n",
    "                    outputs2 = model(torch.flip(images, dims=[3]))\n",
    "                    outputs = (outputs1 + outputs2) / 2\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += images.size(0)\n",
    "\n",
    "            if pbar.n % max(1, len(loader) // 10) == 0:\n",
    "                avg_loss = running_loss / (pbar.n + 1)\n",
    "                avg_acc = 100.0 * correct / total\n",
    "                pbar.set_postfix({\"loss\": f\"{avg_loss:.4f}\", \"acc\": f\"{avg_acc:.2f}%\"})\n",
    "\n",
    "    avg_loss = running_loss / len(loader)\n",
    "    avg_acc = 100.0 * correct / total\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  8. Main Training with OneCycleLR\n",
    "# ============================================================\n",
    "\n",
    "def train_advanced(model, train_loader, val_loader, criterion, \n",
    "                   device, CONFIG, class_names):\n",
    "    \"\"\"\n",
    "    Advanced training with OneCycleLR for better convergence.\n",
    "    \"\"\"\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=CONFIG['lr'],\n",
    "        weight_decay=CONFIG['weight_decay']\n",
    "    )\n",
    "    \n",
    "    # OneCycleLR: proven to reach higher accuracy faster\n",
    "    total_steps = len(train_loader) * CONFIG['epochs']\n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=CONFIG['lr'] * 10,  # Peak LR\n",
    "        total_steps=total_steps,\n",
    "        pct_start=0.3,  # Warmup for 30% of training\n",
    "        anneal_strategy='cos',\n",
    "        div_factor=25.0,\n",
    "        final_div_factor=10000.0\n",
    "    )\n",
    "    \n",
    "    use_amp = torch.cuda.is_available()\n",
    "    scaler = GradScaler() if use_amp else None\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Starting high-accuracy training...\")\n",
    "    if use_amp:\n",
    "        print(\"Mixed Precision: ENABLED\")\n",
    "    else:\n",
    "        print(\"Mixed Precision: DISABLED (CPU mode)\")\n",
    "    print(f\"Target: 75-80% validation accuracy\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for epoch in range(1, CONFIG['epochs'] + 1):\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, epoch,\n",
    "            scaler, use_mixup=CONFIG['use_mixup'], \n",
    "            mixup_alpha=CONFIG['mixup_alpha'], use_amp=use_amp,\n",
    "            use_cutmix=CONFIG['use_cutmix']\n",
    "        )\n",
    "        \n",
    "        # Use TTA after epoch 20 for better validation accuracy\n",
    "        use_tta = CONFIG['use_tta'] and epoch > 20\n",
    "        val_loss, val_acc = evaluate(\n",
    "            model, val_loader, criterion, device, epoch, \n",
    "            use_amp=use_amp, use_tta=use_tta\n",
    "        )\n",
    "        \n",
    "        # Calculate overfitting gap\n",
    "        gap = train_acc - val_acc\n",
    "        \n",
    "        print(f\"\\n[Epoch {epoch}/{CONFIG['epochs']}]\")\n",
    "        print(f\"  Train -> Loss: {train_loss:.4f}  Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val   -> Loss: {val_loss:.4f}  Acc: {val_acc:.2f}%\")\n",
    "        print(f\"  Gap: {gap:.2f}% | LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        if use_tta:\n",
    "            print(f\"  TTA: Enabled\")\n",
    "        \n",
    "        # Step scheduler per batch (OneCycleLR requirement handled in training loop)\n",
    "        # scheduler.step() is called per batch inside train_one_epoch\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0\n",
    "            save_checkpoint(model, optimizer, epoch, val_acc, val_loss, \n",
    "                          class_names, CONFIG)\n",
    "            print(f\"  ✓ New best! Val Acc: {val_acc:.2f}% (Gap: {gap:.2f}%)\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  No improvement ({patience_counter}/{CONFIG['patience']})\")\n",
    "            \n",
    "            if patience_counter >= CONFIG['patience']:\n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(f\"Early stopping triggered\")\n",
    "                print(\"=\"*60)\n",
    "                break\n",
    "    \n",
    "    return best_val_acc, best_epoch\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, val_acc, val_loss, class_names, CONFIG):\n",
    "    \"\"\"Save model checkpoint\"\"\"\n",
    "    best_path = os.path.join(CONFIG['save_dir'], \"best_model.pth\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_acc': val_acc,\n",
    "        'val_loss': val_loss,\n",
    "        'class_names': class_names,\n",
    "        'config': CONFIG\n",
    "    }, best_path)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  9. Main Pipeline\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    # ============= High-Accuracy Configuration =============\n",
    "    CONFIG = {\n",
    "        'data_root': 'C:/adam/AMIT_Diploma/grad_project/archive (1)',\n",
    "        'model_name': 'resnet34',  # ResNet34 for better capacity\n",
    "        'img_size': 224,\n",
    "        'batch_size': 64,  # Smaller batch for better generalization\n",
    "        'epochs': 60,  # Extended training\n",
    "        'lr': 3e-4,  # Lower LR for stability\n",
    "        'weight_decay': 2e-4,  # Stronger regularization\n",
    "        'dropout': 0.5,\n",
    "        'val_size': 0.15,\n",
    "        'random_seed': 42,\n",
    "        'num_workers': 0,\n",
    "        'patience': 12,  # More patience for convergence\n",
    "        'use_class_weights': True,\n",
    "        'use_label_smoothing': True,  # Key for generalization\n",
    "        'label_smooth_eps': 0.1,\n",
    "        'use_mixup': True,\n",
    "        'mixup_alpha': 0.4,  # Stronger mixup\n",
    "        'use_cutmix': True,  # Add CutMix\n",
    "        'use_tta': True,  # Test-Time Augmentation\n",
    "        'save_dir': './checkpoints',\n",
    "        'cache_images': True,\n",
    "    }\n",
    "\n",
    "    # ============= Setup =============\n",
    "    random.seed(CONFIG['random_seed'])\n",
    "    np.random.seed(CONFIG['random_seed'])\n",
    "    torch.manual_seed(CONFIG['random_seed'])\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(CONFIG['random_seed'])\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    os.makedirs(CONFIG['save_dir'], exist_ok=True)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(f\"WARNING: CPU training - will take 6-10 hours\")\n",
    "\n",
    "    # ============= Load Data =============\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Loading dataset...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    image_paths, labels, class_names = gather_image_paths_and_labels(CONFIG['data_root'])\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    print(f\"\\nDataset Statistics:\")\n",
    "    print(f\"  Total images: {len(image_paths)}\")\n",
    "    print(f\"  Classes ({num_classes}): {class_names}\")\n",
    "    \n",
    "    label_counts = Counter(labels)\n",
    "    print(f\"\\n  Class distribution:\")\n",
    "    for cls_idx, cls_name in enumerate(class_names):\n",
    "        print(f\"    {cls_name}: {label_counts[cls_idx]}\")\n",
    "\n",
    "    # ============= Stratified Split =============\n",
    "    print(f\"\\nCreating stratified split (val_size={CONFIG['val_size']})...\")\n",
    "    \n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=CONFIG['val_size'],\n",
    "                                random_state=CONFIG['random_seed'])\n",
    "    indices = np.arange(len(labels))\n",
    "    train_idx, val_idx = next(sss.split(indices, labels))\n",
    "\n",
    "    train_paths = [image_paths[i] for i in train_idx]\n",
    "    train_labels_list = [labels[i] for i in train_idx]\n",
    "    val_paths = [image_paths[i] for i in val_idx]\n",
    "    val_labels_list = [labels[i] for i in val_idx]\n",
    "\n",
    "    print(f\"  Train samples: {len(train_paths)}\")\n",
    "    print(f\"  Val samples: {len(val_paths)}\")\n",
    "\n",
    "    # ============= Create Datasets =============\n",
    "    train_tf, val_tf = get_augmentation_transforms()\n",
    "    \n",
    "    train_dataset = PreCachedImageDataset(\n",
    "        train_paths, train_labels_list, \n",
    "        transform=train_tf, \n",
    "        cache_images=CONFIG['cache_images'],\n",
    "        img_size=CONFIG['img_size'],\n",
    "        is_train=True\n",
    "    )\n",
    "    val_dataset = PreCachedImageDataset(\n",
    "        val_paths, val_labels_list,\n",
    "        transform=None,\n",
    "        cache_images=CONFIG['cache_images'],\n",
    "        img_size=CONFIG['img_size'],\n",
    "        is_train=False\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "    )\n",
    "\n",
    "    # ============= Build Enhanced Model =============\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Building enhanced model...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    model = build_model(\n",
    "        model_name=CONFIG['model_name'],\n",
    "        num_classes=num_classes,\n",
    "        dropout_rate=CONFIG['dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"  Model: {CONFIG['model_name']}\")\n",
    "    print(f\"  Architecture: {CONFIG['model_name'].upper()} + 3-layer classifier\")\n",
    "    print(f\"  Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "    # ============= Loss with Label Smoothing =============\n",
    "    if CONFIG['use_class_weights']:\n",
    "        train_counts = np.bincount(train_labels_list, minlength=num_classes)\n",
    "        class_weights = 1.0 / (train_counts + 1e-6)\n",
    "        class_weights = class_weights * (len(train_labels_list) / class_weights.sum())\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "        print(f\"\\n  Class weights: {class_weights.cpu().numpy()}\")\n",
    "    else:\n",
    "        class_weights = None\n",
    "    \n",
    "    if CONFIG['use_label_smoothing']:\n",
    "        criterion = LabelSmoothingCrossEntropy(\n",
    "            epsilon=CONFIG['label_smooth_eps'], \n",
    "            weight=class_weights\n",
    "        )\n",
    "        print(f\"  Loss: Label Smoothing CE (eps={CONFIG['label_smooth_eps']})\")\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        print(f\"  Loss: Standard Cross Entropy\")\n",
    "\n",
    "    # ============= Training =============\n",
    "    best_val_acc, best_epoch = train_advanced(\n",
    "        model, train_loader, val_loader, criterion, device, CONFIG, class_names\n",
    "    )\n",
    "\n",
    "    # ============= Final Results =============\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Training Complete!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"  Best Val Acc: {best_val_acc:.2f}% (Epoch {best_epoch})\")\n",
    "    print(f\"  Target Range: 75-80%\")\n",
    "    if best_val_acc >= 75:\n",
    "        print(f\"  ✓ TARGET ACHIEVED!\")\n",
    "    else:\n",
    "        print(f\"  Gap to target: {75 - best_val_acc:.2f}%\")\n",
    "    print(f\"  Model saved to: {CONFIG['save_dir']}/best_model.pth\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90265467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea15054d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 30 17:03:01 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 581.29                 Driver Version: 581.29         CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   36C    P0              9W /   80W |       0MiB /   8188MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a042307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bebb23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_grad (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
