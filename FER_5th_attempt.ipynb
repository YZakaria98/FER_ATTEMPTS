{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f275860c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOptimized FER Training Pipeline with:\\n - Fast in-memory data loading (pre-cached images)\\n - Stratified train/val split\\n - Modern PyTorch APIs\\n - Efficient data augmentation\\n - Early stopping and learning rate scheduling\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Optimized FER Training Pipeline with:\n",
    " - Fast in-memory data loading (pre-cached images)\n",
    " - Stratified train/val split\n",
    " - Modern PyTorch APIs\n",
    " - Efficient data augmentation\n",
    " - Early stopping and learning rate scheduling\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b809da67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36e37aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  1. Data Collection\n",
    "# ============================================================\n",
    "\n",
    "def gather_image_paths_and_labels(root_dir: str) -> Tuple[List[str], List[int], List[str]]:\n",
    "    \"\"\"\n",
    "    Collect all images from emotion class subdirectories.\n",
    "    Handles both structures:\n",
    "      - root/emotion/*.jpg\n",
    "      - root/train/emotion/*.jpg and root/test/emotion/*.jpg\n",
    "    \n",
    "    Returns:\n",
    "        image_paths: List of file paths\n",
    "        labels: List of integer labels\n",
    "        class_names: List of class names (sorted)\n",
    "    \"\"\"\n",
    "    root = Path(root_dir)\n",
    "    if not root.exists():\n",
    "        raise FileNotFoundError(f\"Dataset root not found: {root_dir}\")\n",
    "\n",
    "    # Collect all images with their parent directory (emotion class)\n",
    "    image_data = []\n",
    "    exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\"}\n",
    "    \n",
    "    for img_path in root.rglob(\"*\"):\n",
    "        if img_path.suffix.lower() in exts:\n",
    "            # Get emotion class (parent directory name)\n",
    "            emotion_class = img_path.parent.name\n",
    "            # Skip if parent is 'train' or 'test' - go up one more level\n",
    "            if emotion_class in ['train', 'test']:\n",
    "                emotion_class = img_path.parent.parent.name\n",
    "            \n",
    "            image_data.append((str(img_path), emotion_class))\n",
    "    \n",
    "    if not image_data:\n",
    "        raise ValueError(f\"No images found under {root_dir}\")\n",
    "    \n",
    "    # Create class mapping\n",
    "    unique_classes = sorted(set(emotion for _, emotion in image_data))\n",
    "    class_to_idx = {name: idx for idx, name in enumerate(unique_classes)}\n",
    "    \n",
    "    # Convert to lists\n",
    "    image_paths = [path for path, _ in image_data]\n",
    "    labels = [class_to_idx[emotion] for _, emotion in image_data]\n",
    "    \n",
    "    return image_paths, labels, unique_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b920361",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  2. Pre-cached Dataset\n",
    "# ============================================================\n",
    "\n",
    "class PreCachedImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Ultra-fast dataset that stores pre-processed tensors in memory.\n",
    "    For training: applies minimal random augmentation on cached tensors.\n",
    "    For validation: returns tensors directly (no augmentation).\n",
    "    \"\"\"\n",
    "    def __init__(self, image_paths: List[str], labels: List[int], \n",
    "                 transform=None, cache_images: bool = True, img_size: int = 224,\n",
    "                 is_train: bool = True):\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)  # Pre-convert labels to tensor\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "        self.cached_tensors = []\n",
    "        \n",
    "        if cache_images:\n",
    "            print(f\"Pre-loading and processing {len(image_paths)} images...\")\n",
    "            \n",
    "            # Pre-process transform (applied once during caching)\n",
    "            imagenet_mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "            imagenet_std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "            \n",
    "            resize_transform = transforms.Compose([\n",
    "                transforms.Resize(img_size),\n",
    "                transforms.ToTensor(),\n",
    "            ])\n",
    "            \n",
    "            for path in tqdm(image_paths, desc=\"Caching\"):\n",
    "                try:\n",
    "                    img = Image.open(path).convert(\"RGB\")\n",
    "                    img_tensor = resize_transform(img)\n",
    "                    \n",
    "                    # For validation, pre-normalize to avoid doing it on every access\n",
    "                    if not is_train:\n",
    "                        img_tensor = (img_tensor - imagenet_mean) / imagenet_std\n",
    "                    \n",
    "                    self.cached_tensors.append(img_tensor)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Failed to load {path}: {e}\")\n",
    "                    blank = torch.zeros(3, img_size, img_size)\n",
    "                    if not is_train:\n",
    "                        blank = (blank - imagenet_mean) / imagenet_std\n",
    "                    self.cached_tensors.append(blank)\n",
    "        else:\n",
    "            self.cached_tensors = None\n",
    "            self.image_paths = image_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.cached_tensors is not None:\n",
    "            img = self.cached_tensors[idx]\n",
    "            \n",
    "            # For training, apply augmentation\n",
    "            if self.is_train and self.transform:\n",
    "                img = self.transform(img)\n",
    "            # For validation, tensor is already normalized, return as-is\n",
    "            \n",
    "        else:\n",
    "            img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "        \n",
    "        return img, self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13c77af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  3. Transforms\n",
    "# ============================================================\n",
    "\n",
    "def get_augmentation_transforms() -> Tuple[transforms.Compose, transforms.Compose]:\n",
    "    \"\"\"\n",
    "    Returns lightweight augmentation transforms for pre-cached tensors.\n",
    "    Applied on GPU-ready tensors for speed.\n",
    "    \"\"\"\n",
    "    imagenet_mean = [0.485, 0.456, 0.406]\n",
    "    imagenet_std = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    # Train: augmentation on tensors (fast)\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.Normalize(imagenet_mean, imagenet_std),\n",
    "        transforms.RandomErasing(p=0.1, scale=(0.02, 0.1)),  # Applied after normalize\n",
    "    ])\n",
    "    \n",
    "    # Val: just normalize\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Normalize(imagenet_mean, imagenet_std),\n",
    "    ])\n",
    "    \n",
    "    return train_transform, val_transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f724fe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  4. Model Builder\n",
    "# ============================================================\n",
    "\n",
    "def build_model(model_name: str = 'resnet18', num_classes: int = 7, \n",
    "                dropout_rate: float = 0.5, freeze_backbone: bool = False) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Build ResNet-based classifier with modern PyTorch API.\n",
    "    \"\"\"\n",
    "    if model_name == 'resnet18':\n",
    "        weights = models.ResNet18_Weights.IMAGENET1K_V1\n",
    "        model = models.resnet18(weights=weights)\n",
    "    elif model_name == 'resnet50':\n",
    "        weights = models.ResNet50_Weights.IMAGENET1K_V1\n",
    "        model = models.resnet50(weights=weights)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "\n",
    "    # Replace classifier head\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(dropout_rate / 2),  # Light dropout before first layer\n",
    "        nn.Linear(in_features, 512),\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(dropout_rate),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.BatchNorm1d(256),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(dropout_rate),\n",
    "        nn.Linear(256, num_classes)\n",
    "    )\n",
    "\n",
    "    # Freeze backbone if requested\n",
    "    if freeze_backbone:\n",
    "        for name, param in model.named_parameters():\n",
    "            if \"fc\" not in name:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86c4d73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  5. Training Functions\n",
    "# ============================================================\n",
    "\n",
    "def train_one_epoch(model: nn.Module, loader: DataLoader, criterion: nn.Module,\n",
    "                   optimizer: optim.Optimizer, device: torch.device, epoch: int) -> Tuple[float, float]:\n",
    "    \"\"\"Train for one epoch with optimized data loading\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch} [Train]\", leave=False)\n",
    "\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)  # Faster than zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += images.size(0)\n",
    "\n",
    "        # Update progress bar less frequently for speed\n",
    "        if total % (images.size(0) * 5) == 0:  # Update every 5 batches\n",
    "            avg_loss = running_loss / total\n",
    "            avg_acc = 100.0 * correct / total\n",
    "            pbar.set_postfix({\"loss\": f\"{avg_loss:.4f}\", \"acc\": f\"{avg_acc:.2f}%\"})\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    avg_acc = 100.0 * correct / total\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader, criterion: nn.Module,\n",
    "            device: torch.device, epoch: int) -> Tuple[float, float]:\n",
    "    \"\"\"Evaluate on validation set\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch} [Val]\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in pbar:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += images.size(0)\n",
    "\n",
    "            # Update progress bar\n",
    "            avg_loss = running_loss / total\n",
    "            avg_acc = 100.0 * correct / total\n",
    "            pbar.set_postfix({\"loss\": f\"{avg_loss:.4f}\", \"acc\": f\"{avg_acc:.2f}%\"})\n",
    "\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4455cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "============================================================\n",
      "Loading dataset...\n",
      "============================================================\n",
      "\n",
      "Dataset Statistics:\n",
      "  Total images: 35887\n",
      "  Classes (7): ['angry', 'disgusted', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n",
      "\n",
      "  Class distribution:\n",
      "    angry: 4953\n",
      "    disgusted: 547\n",
      "    fearful: 5121\n",
      "    happy: 8989\n",
      "    neutral: 6198\n",
      "    sad: 6077\n",
      "    surprised: 4002\n",
      "\n",
      "Creating stratified split (val_size=0.15)...\n",
      "  Train samples: 30503\n",
      "  Val samples: 5384\n",
      "Pre-loading and processing 30503 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching: 100%|██████████| 30503/30503 [00:40<00:00, 755.21it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loading and processing 5384 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching: 100%|██████████| 5384/5384 [00:11<00:00, 469.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Building model...\n",
      "============================================================\n",
      "  Model: resnet18\n",
      "  Freeze backbone: False\n",
      "  Dropout: 0.5\n",
      "  Trainable params: 11,573,831\n",
      "\n",
      "  Using class weights: [ 2114.7783 19146.703   2045.306   1165.3425  1690.0564  1723.7594\n",
      "  2617.0537]\n",
      "\n",
      "============================================================\n",
      "Starting training...\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 1/30]\n",
      "  Train -> Loss: 1.6421  Acc: 37.63%\n",
      "  Val   -> Loss: 1.4565  Acc: 39.67%\n",
      "  ✓ New best model saved! (Val Acc: 39.67%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 2/30]\n",
      "  Train -> Loss: 1.3898  Acc: 47.78%\n",
      "  Val   -> Loss: 1.3160  Acc: 52.14%\n",
      "  ✓ New best model saved! (Val Acc: 52.14%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 3/30]\n",
      "  Train -> Loss: 1.2659  Acc: 52.27%\n",
      "  Val   -> Loss: 1.2822  Acc: 54.62%\n",
      "  ✓ New best model saved! (Val Acc: 54.62%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 4/30]\n",
      "  Train -> Loss: 1.2051  Acc: 54.13%\n",
      "  Val   -> Loss: 1.2436  Acc: 55.55%\n",
      "  ✓ New best model saved! (Val Acc: 55.55%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 5/30]\n",
      "  Train -> Loss: 1.1437  Acc: 56.50%\n",
      "  Val   -> Loss: 1.1804  Acc: 55.20%\n",
      "  No improvement (1/7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 6/30]\n",
      "  Train -> Loss: 1.0583  Acc: 59.37%\n",
      "  Val   -> Loss: 1.1982  Acc: 57.56%\n",
      "  ✓ New best model saved! (Val Acc: 57.56%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 7/30]\n",
      "  Train -> Loss: 1.0505  Acc: 60.10%\n",
      "  Val   -> Loss: 1.0719  Acc: 57.80%\n",
      "  ✓ New best model saved! (Val Acc: 57.80%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 8/30]\n",
      "  Train -> Loss: 0.9787  Acc: 62.21%\n",
      "  Val   -> Loss: 1.1655  Acc: 58.08%\n",
      "  ✓ New best model saved! (Val Acc: 58.08%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 9/30]\n",
      "  Train -> Loss: 0.9496  Acc: 63.68%\n",
      "  Val   -> Loss: 1.0744  Acc: 61.70%\n",
      "  ✓ New best model saved! (Val Acc: 61.70%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 10/30]\n",
      "  Train -> Loss: 0.8839  Acc: 66.56%\n",
      "  Val   -> Loss: 1.1152  Acc: 63.32%\n",
      "  ✓ New best model saved! (Val Acc: 63.32%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 11/30]\n",
      "  Train -> Loss: 0.8308  Acc: 67.56%\n",
      "  Val   -> Loss: 1.1231  Acc: 61.79%\n",
      "  No improvement (1/7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 12/30]\n",
      "  Train -> Loss: 0.7007  Acc: 73.10%\n",
      "  Val   -> Loss: 1.0381  Acc: 64.71%\n",
      "  ✓ New best model saved! (Val Acc: 64.71%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 13/30]\n",
      "  Train -> Loss: 0.6287  Acc: 75.92%\n",
      "  Val   -> Loss: 1.0434  Acc: 63.87%\n",
      "  No improvement (1/7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 14/30]\n",
      "  Train -> Loss: 0.5546  Acc: 78.95%\n",
      "  Val   -> Loss: 1.1433  Acc: 63.06%\n",
      "  No improvement (2/7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 15/30]\n",
      "  Train -> Loss: 0.4769  Acc: 82.20%\n",
      "  Val   -> Loss: 1.1997  Acc: 63.47%\n",
      "  No improvement (3/7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 16/30]\n",
      "  Train -> Loss: 0.4327  Acc: 84.20%\n",
      "  Val   -> Loss: 1.2726  Acc: 64.28%\n",
      "  No improvement (4/7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 17/30]\n",
      "  Train -> Loss: 0.3190  Acc: 88.75%\n",
      "  Val   -> Loss: 1.2956  Acc: 65.21%\n",
      "  ✓ New best model saved! (Val Acc: 65.21%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 18/30]\n",
      "  Train -> Loss: 0.2405  Acc: 91.77%\n",
      "  Val   -> Loss: 1.4284  Acc: 65.62%\n",
      "  ✓ New best model saved! (Val Acc: 65.62%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 19/30]\n",
      "  Train -> Loss: 0.1973  Acc: 93.41%\n",
      "  Val   -> Loss: 1.4902  Acc: 65.32%\n",
      "  No improvement (1/7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 20/30]\n",
      "  Train -> Loss: 0.1693  Acc: 94.44%\n",
      "  Val   -> Loss: 1.6254  Acc: 64.62%\n",
      "  No improvement (2/7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 21/30]\n",
      "  Train -> Loss: 0.1258  Acc: 95.90%\n",
      "  Val   -> Loss: 1.6347  Acc: 65.42%\n",
      "  No improvement (3/7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 22/30]\n",
      "  Train -> Loss: 0.1099  Acc: 96.63%\n",
      "  Val   -> Loss: 1.6134  Acc: 65.64%\n",
      "  ✓ New best model saved! (Val Acc: 65.64%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 23/30]\n",
      "  Train -> Loss: 0.0971  Acc: 97.04%\n",
      "  Val   -> Loss: 1.7265  Acc: 65.38%\n",
      "  No improvement (1/7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 24/30]\n",
      "  Train -> Loss: 0.0825  Acc: 97.54%\n",
      "  Val   -> Loss: 1.7945  Acc: 65.27%\n",
      "  No improvement (2/7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 25/30]\n",
      "  Train -> Loss: 0.0643  Acc: 98.14%\n",
      "  Val   -> Loss: 1.8197  Acc: 65.73%\n",
      "  ✓ New best model saved! (Val Acc: 65.73%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 26/30]\n",
      "  Train -> Loss: 0.0587  Acc: 98.27%\n",
      "  Val   -> Loss: 1.8484  Acc: 65.81%\n",
      "  ✓ New best model saved! (Val Acc: 65.81%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 27/30]\n",
      "  Train -> Loss: 0.0558  Acc: 98.34%\n",
      "  Val   -> Loss: 1.8597  Acc: 65.82%\n",
      "  ✓ New best model saved! (Val Acc: 65.82%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 28/30]\n",
      "  Train -> Loss: 0.0504  Acc: 98.49%\n",
      "  Val   -> Loss: 1.9368  Acc: 65.86%\n",
      "  ✓ New best model saved! (Val Acc: 65.86%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 29/30]\n",
      "  Train -> Loss: 0.0466  Acc: 98.67%\n",
      "  Val   -> Loss: 1.8468  Acc: 66.10%\n",
      "  ✓ New best model saved! (Val Acc: 66.10%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 30/30]\n",
      "  Train -> Loss: 0.0419  Acc: 98.77%\n",
      "  Val   -> Loss: 1.9034  Acc: 66.08%\n",
      "  No improvement (1/7)\n",
      "\n",
      "============================================================\n",
      "Training Complete!\n",
      "============================================================\n",
      "  Best Val Acc: 66.10% (Epoch 29)\n",
      "  Model saved to: ./checkpoints/best_model.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  6. Main Training Pipeline\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    # ============= Configuration =============\n",
    "    CONFIG = {\n",
    "        'data_root': r\"C:\\Users\\be724\\Downloads\\archive (3)\",\n",
    "        'model_name': 'resnet18',  # or 'resnet50'\n",
    "        'img_size': 112,\n",
    "        'batch_size': 64,  # Large batch size works great with cached data\n",
    "        'epochs': 30,\n",
    "        'lr': 1e-3,  # Higher initial LR (will be reduced by scheduler)\n",
    "        'weight_decay': 1e-4,\n",
    "        'dropout': 0.5,\n",
    "        'freeze_backbone': False,\n",
    "        'val_size': 0.15,\n",
    "        'random_seed': 42,\n",
    "        'num_workers': 0,  # Must be 0 for cached tensor datasets\n",
    "        'patience': 7,\n",
    "        'use_class_weights': True,\n",
    "        'save_dir': './checkpoints',\n",
    "        'cache_images': True,  # Pre-load images into RAM\n",
    "    }\n",
    "\n",
    "    # ============= Setup =============\n",
    "    random.seed(CONFIG['random_seed'])\n",
    "    np.random.seed(CONFIG['random_seed'])\n",
    "    torch.manual_seed(CONFIG['random_seed'])\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(CONFIG['random_seed'])\n",
    "        torch.backends.cudnn.benchmark = True  # Optimize for fixed input size\n",
    "\n",
    "    os.makedirs(CONFIG['save_dir'], exist_ok=True)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # ============= Load Data =============\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Loading dataset...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    image_paths, labels, class_names = gather_image_paths_and_labels(CONFIG['data_root'])\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    print(f\"\\nDataset Statistics:\")\n",
    "    print(f\"  Total images: {len(image_paths)}\")\n",
    "    print(f\"  Classes ({num_classes}): {class_names}\")\n",
    "    \n",
    "    # Class distribution\n",
    "    label_counts = Counter(labels)\n",
    "    print(f\"\\n  Class distribution:\")\n",
    "    for cls_idx, cls_name in enumerate(class_names):\n",
    "        print(f\"    {cls_name}: {label_counts[cls_idx]}\")\n",
    "\n",
    "    # ============= Stratified Split =============\n",
    "    print(f\"\\nCreating stratified split (val_size={CONFIG['val_size']})...\")\n",
    "    \n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=CONFIG['val_size'],\n",
    "                                random_state=CONFIG['random_seed'])\n",
    "    indices = np.arange(len(labels))\n",
    "    train_idx, val_idx = next(sss.split(indices, labels))\n",
    "\n",
    "    train_paths = [image_paths[i] for i in train_idx]\n",
    "    train_labels_list = [labels[i] for i in train_idx]\n",
    "    val_paths = [image_paths[i] for i in val_idx]\n",
    "    val_labels_list = [labels[i] for i in val_idx]\n",
    "\n",
    "    print(f\"  Train samples: {len(train_paths)}\")\n",
    "    print(f\"  Val samples: {len(val_paths)}\")\n",
    "\n",
    "    # ============= Create Datasets =============\n",
    "    train_tf, val_tf = get_augmentation_transforms()\n",
    "    \n",
    "    train_dataset = PreCachedImageDataset(\n",
    "        train_paths, train_labels_list, \n",
    "        transform=train_tf, \n",
    "        cache_images=CONFIG['cache_images'],\n",
    "        img_size=CONFIG['img_size'],\n",
    "        is_train=True\n",
    "    )\n",
    "    val_dataset = PreCachedImageDataset(\n",
    "        val_paths, val_labels_list,\n",
    "        transform=None,  # Validation is pre-normalized\n",
    "        cache_images=CONFIG['cache_images'],\n",
    "        img_size=CONFIG['img_size'],\n",
    "        is_train=False\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # CRITICAL: Use 0 workers with cached data to avoid pickle overhead\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=0,  # CRITICAL: Use 0 workers with cached data\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "    )\n",
    "\n",
    "    # ============= Build Model =============\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Building model...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    model = build_model(\n",
    "        model_name=CONFIG['model_name'],\n",
    "        num_classes=num_classes,\n",
    "        dropout_rate=CONFIG['dropout'],\n",
    "        freeze_backbone=CONFIG['freeze_backbone']\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"  Model: {CONFIG['model_name']}\")\n",
    "    print(f\"  Freeze backbone: {CONFIG['freeze_backbone']}\")\n",
    "    print(f\"  Dropout: {CONFIG['dropout']}\")\n",
    "    print(f\"  Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "    # ============= Loss & Optimizer =============\n",
    "    if CONFIG['use_class_weights']:\n",
    "        train_counts = np.bincount(train_labels_list, minlength=num_classes)\n",
    "        class_weights = 1.0 / (train_counts + 1e-6)\n",
    "        class_weights = class_weights * (len(train_labels_list) / class_weights.sum())\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        print(f\"\\n  Using class weights: {class_weights.cpu().numpy()}\")\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=CONFIG['lr'],\n",
    "        weight_decay=CONFIG['weight_decay']\n",
    "    )\n",
    "    \n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=3\n",
    "    )\n",
    "\n",
    "    # ============= Training Loop =============\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Starting training...\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(1, CONFIG['epochs'] + 1):\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, epoch\n",
    "        )\n",
    "        val_loss, val_acc = evaluate(\n",
    "            model, val_loader, criterion, device, epoch\n",
    "        )\n",
    "\n",
    "        print(f\"\\n[Epoch {epoch}/{CONFIG['epochs']}]\")\n",
    "        print(f\"  Train -> Loss: {train_loss:.4f}  Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val   -> Loss: {val_loss:.4f}  Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0\n",
    "            \n",
    "            best_path = os.path.join(CONFIG['save_dir'], \"best_model.pth\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "                'val_loss': val_loss,\n",
    "                'class_names': class_names,\n",
    "                'config': CONFIG\n",
    "            }, best_path)\n",
    "            print(f\"  ✓ New best model saved! (Val Acc: {val_acc:.2f}%)\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  No improvement ({patience_counter}/{CONFIG['patience']})\")\n",
    "            \n",
    "            if patience_counter >= CONFIG['patience']:\n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(f\"Early stopping triggered (no improvement for {CONFIG['patience']} epochs)\")\n",
    "                print(\"=\"*60)\n",
    "                break\n",
    "\n",
    "    # ============= Training Complete =============\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Training Complete!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"  Best Val Acc: {best_val_acc:.2f}% (Epoch {best_epoch})\")\n",
    "    print(f\"  Model saved to: {CONFIG['save_dir']}/best_model.pth\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0971d630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d126625",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
